{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)\n",
    "\n",
    "val_split = 0.2\n",
    "\n",
    "train_indices, val_indices = train_test_split(list(range(len(train_data))), test_size=val_split, random_state=42)\n",
    "\n",
    "train_subset = Subset(train_data, train_indices)\n",
    "val_subset = Subset(train_data, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "steps = len(train_loader)\n",
    "\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_loss = np.Inf\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 2.2584, Validation Loss: 2.0464\n",
      "Epoch [2/50], Training Loss: 1.8763, Validation Loss: 1.7203\n",
      "Epoch [3/50], Training Loss: 1.6452, Validation Loss: 1.5644\n",
      "Epoch [4/50], Training Loss: 1.5322, Validation Loss: 1.4910\n",
      "Epoch [5/50], Training Loss: 1.4505, Validation Loss: 1.3862\n",
      "Epoch [6/50], Training Loss: 1.3758, Validation Loss: 1.3369\n",
      "Epoch [7/50], Training Loss: 1.3094, Validation Loss: 1.2494\n",
      "Epoch [8/50], Training Loss: 1.2562, Validation Loss: 1.2022\n",
      "Epoch [9/50], Training Loss: 1.2092, Validation Loss: 1.1510\n",
      "Epoch [10/50], Training Loss: 1.1701, Validation Loss: 1.1201\n",
      "Epoch [11/50], Training Loss: 1.1335, Validation Loss: 1.1136\n",
      "Epoch [12/50], Training Loss: 1.1004, Validation Loss: 1.0486\n",
      "Epoch [13/50], Training Loss: 1.0707, Validation Loss: 1.0315\n",
      "Epoch [14/50], Training Loss: 1.0424, Validation Loss: 1.0012\n",
      "Epoch [15/50], Training Loss: 1.0152, Validation Loss: 0.9648\n",
      "Epoch [16/50], Training Loss: 0.9886, Validation Loss: 0.9419\n",
      "Epoch [17/50], Training Loss: 0.9659, Validation Loss: 0.9183\n",
      "Epoch [18/50], Training Loss: 0.9422, Validation Loss: 0.8957\n",
      "Epoch [19/50], Training Loss: 0.9208, Validation Loss: 0.8643\n",
      "Epoch [20/50], Training Loss: 0.8987, Validation Loss: 0.8452\n",
      "Epoch [21/50], Training Loss: 0.8802, Validation Loss: 0.8360\n",
      "Epoch [22/50], Training Loss: 0.8607, Validation Loss: 0.8258\n",
      "Epoch [23/50], Training Loss: 0.8417, Validation Loss: 0.8064\n",
      "Epoch [24/50], Training Loss: 0.8250, Validation Loss: 0.7762\n",
      "Epoch [25/50], Training Loss: 0.8083, Validation Loss: 0.7479\n",
      "Epoch [26/50], Training Loss: 0.7876, Validation Loss: 0.7407\n",
      "Epoch [27/50], Training Loss: 0.7742, Validation Loss: 0.7266\n",
      "Epoch [28/50], Training Loss: 0.7569, Validation Loss: 0.7080\n",
      "Epoch [29/50], Training Loss: 0.7411, Validation Loss: 0.6813\n",
      "Epoch [30/50], Training Loss: 0.7257, Validation Loss: 0.6620\n",
      "Epoch [31/50], Training Loss: 0.7098, Validation Loss: 0.6532\n",
      "Epoch [32/50], Training Loss: 0.6952, Validation Loss: 0.6451\n",
      "Epoch [33/50], Training Loss: 0.6821, Validation Loss: 0.6198\n",
      "Epoch [34/50], Training Loss: 0.6658, Validation Loss: 0.6103\n",
      "Epoch [35/50], Training Loss: 0.6525, Validation Loss: 0.5980\n",
      "Epoch [36/50], Training Loss: 0.6370, Validation Loss: 0.6029\n",
      "Epoch [37/50], Training Loss: 0.6243, Validation Loss: 0.5850\n",
      "Epoch [38/50], Training Loss: 0.6116, Validation Loss: 0.5378\n",
      "Epoch [39/50], Training Loss: 0.5971, Validation Loss: 0.5262\n",
      "Epoch [40/50], Training Loss: 0.5838, Validation Loss: 0.5333\n",
      "Epoch [41/50], Training Loss: 0.5706, Validation Loss: 0.5141\n",
      "Epoch [42/50], Training Loss: 0.5584, Validation Loss: 0.5362\n",
      "Epoch [43/50], Training Loss: 0.5461, Validation Loss: 0.4918\n",
      "Epoch [44/50], Training Loss: 0.5335, Validation Loss: 0.4817\n",
      "Epoch [45/50], Training Loss: 0.5215, Validation Loss: 0.4865\n",
      "Epoch [46/50], Training Loss: 0.5106, Validation Loss: 0.4593\n",
      "Epoch [47/50], Training Loss: 0.4982, Validation Loss: 0.4509\n",
      "Epoch [48/50], Training Loss: 0.4843, Validation Loss: 0.4135\n",
      "Epoch [49/50], Training Loss: 0.4740, Validation Loss: 0.4257\n",
      "Epoch [50/50], Training Loss: 0.4628, Validation Loss: 0.3776\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #if (i+1) % 2000 == 0:\n",
    "        #    print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        counter = 0  \n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    num_correct_class = [0 for i in range(10)]\n",
    "    num_class_samples = [0 for i in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        num_samples += labels.size(0)\n",
    "        num_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "\n",
    "            if (label == pred):\n",
    "                num_correct_class[label] += 1\n",
    "            \n",
    "            num_class_samples[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.48 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 * num_correct / num_samples\n",
    "print(f'Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane: 70.2 %\n",
      "Accuracy of car: 77.9 %\n",
      "Accuracy of bird: 52.6 %\n",
      "Accuracy of cat: 42.3 %\n",
      "Accuracy of deer: 57.7 %\n",
      "Accuracy of dog: 50.5 %\n",
      "Accuracy of frog: 72.4 %\n",
      "Accuracy of horse: 70.2 %\n",
      "Accuracy of ship: 78.7 %\n",
      "Accuracy of truck: 72.3 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    accuracy = 100 * num_correct_class[i] / num_class_samples[i]\n",
    "    print(f'Accuracy of {classes[i]}: {accuracy} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
